{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411807b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:36.651956Z",
     "iopub.status.busy": "2023-05-27T22:33:36.651611Z",
     "iopub.status.idle": "2023-05-27T22:33:37.441366Z",
     "shell.execute_reply": "2023-05-27T22:33:37.440167Z"
    },
    "papermill": {
     "duration": 0.798707,
     "end_time": "2023-05-27T22:33:37.443522",
     "exception": false,
     "start_time": "2023-05-27T22:33:36.644815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "huggingface_hub.login(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n",
    "%env WANDB_API_KEY={user_secrets.get_secret(\"WANDB_API_KEY\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea69d32",
   "metadata": {
    "papermill": {
     "duration": 0.004971,
     "end_time": "2023-05-27T22:33:37.453836",
     "exception": false,
     "start_time": "2023-05-27T22:33:37.448865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625744c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:37.466414Z",
     "iopub.status.busy": "2023-05-27T22:33:37.464922Z",
     "iopub.status.idle": "2023-05-27T22:33:39.637821Z",
     "shell.execute_reply": "2023-05-27T22:33:39.636975Z"
    },
    "papermill": {
     "duration": 2.181548,
     "end_time": "2023-05-27T22:33:39.640390",
     "exception": false,
     "start_time": "2023-05-27T22:33:37.458842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Audio, ClassLabel, load_dataset\n",
    "from datasets import IterableDatasetDict\n",
    "\n",
    "dusha = IterableDatasetDict()\n",
    "\n",
    "dusha[\"train\"] = load_dataset(\"KELONMYOSA/dusha_emotion_audio\", split=\"train\", streaming=True)\n",
    "dusha[\"test\"] = load_dataset(\"KELONMYOSA/dusha_emotion_audio\", split=\"test\", streaming=True)\n",
    "\n",
    "labels = dusha[\"train\"].features[\"label\"].names\n",
    "\n",
    "dusha = dusha.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "dusha = dusha.remove_columns(\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101d28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:39.652589Z",
     "iopub.status.busy": "2023-05-27T22:33:39.652120Z",
     "iopub.status.idle": "2023-05-27T22:33:39.658068Z",
     "shell.execute_reply": "2023-05-27T22:33:39.657207Z"
    },
    "papermill": {
     "duration": 0.014463,
     "end_time": "2023-05-27T22:33:39.660259",
     "exception": false,
     "start_time": "2023-05-27T22:33:39.645796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0610e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:39.672119Z",
     "iopub.status.busy": "2023-05-27T22:33:39.671407Z",
     "iopub.status.idle": "2023-05-27T22:33:45.616682Z",
     "shell.execute_reply": "2023-05-27T22:33:45.615757Z"
    },
    "papermill": {
     "duration": 5.956632,
     "end_time": "2023-05-27T22:33:45.621976",
     "exception": false,
     "start_time": "2023-05-27T22:33:39.665344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-xls-r-1b-russian\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = processor(audio_arrays, sampling_rate=processor.feature_extractor.sampling_rate,\n",
    "                       return_tensors=\"pt\", padding=True)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9396f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:45.636885Z",
     "iopub.status.busy": "2023-05-27T22:33:45.635752Z",
     "iopub.status.idle": "2023-05-27T22:33:45.642486Z",
     "shell.execute_reply": "2023-05-27T22:33:45.641648Z"
    },
    "papermill": {
     "duration": 0.016361,
     "end_time": "2023-05-27T22:33:45.644571",
     "exception": false,
     "start_time": "2023-05-27T22:33:45.628210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dusha = dusha.map(preprocess_function, remove_columns=\"audio\", batched=True, batch_size=256).with_format(\"torch\")\n",
    "dusha[\"train\"] = dusha[\"train\"].shuffle(buffer_size=256, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46fa45",
   "metadata": {
    "papermill": {
     "duration": 0.005605,
     "end_time": "2023-05-27T22:33:45.656177",
     "exception": false,
     "start_time": "2023-05-27T22:33:45.650572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588dd86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:45.669943Z",
     "iopub.status.busy": "2023-05-27T22:33:45.669006Z",
     "iopub.status.idle": "2023-05-27T22:33:58.461477Z",
     "shell.execute_reply": "2023-05-27T22:33:58.460161Z"
    },
    "papermill": {
     "duration": 12.801636,
     "end_time": "2023-05-27T22:33:58.463902",
     "exception": false,
     "start_time": "2023-05-27T22:33:45.662266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8216b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:33:58.479531Z",
     "iopub.status.busy": "2023-05-27T22:33:58.479225Z",
     "iopub.status.idle": "2023-05-27T22:34:07.231890Z",
     "shell.execute_reply": "2023-05-27T22:34:07.230774Z"
    },
    "papermill": {
     "duration": 8.763588,
     "end_time": "2023-05-27T22:34:07.234429",
     "exception": false,
     "start_time": "2023-05-27T22:33:58.470841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List, Optional, Union, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from packaging import version\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import Wav2Vec2Processor, Trainer, is_apex_available\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2PreTrainedModel, Wav2Vec2Model\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=\"mean\")\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"label\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5cbc4",
   "metadata": {
    "papermill": {
     "duration": 0.006566,
     "end_time": "2023-05-27T22:34:07.247994",
     "exception": false,
     "start_time": "2023-05-27T22:34:07.241428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946feeac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:34:07.262766Z",
     "iopub.status.busy": "2023-05-27T22:34:07.262471Z",
     "iopub.status.idle": "2023-05-27T22:34:07.270043Z",
     "shell.execute_reply": "2023-05-27T22:34:07.269225Z"
    },
    "papermill": {
     "duration": 0.017574,
     "end_time": "2023-05-27T22:34:07.272383",
     "exception": false,
     "start_time": "2023-05-27T22:34:07.254809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "is_regression = False\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a595b",
   "metadata": {
    "papermill": {
     "duration": 0.006614,
     "end_time": "2023-05-27T22:34:07.285553",
     "exception": false,
     "start_time": "2023-05-27T22:34:07.278939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62f4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:34:07.299910Z",
     "iopub.status.busy": "2023-05-27T22:34:07.299637Z",
     "iopub.status.idle": "2023-05-27T22:34:18.475283Z",
     "shell.execute_reply": "2023-05-27T22:34:18.474345Z"
    },
    "papermill": {
     "duration": 11.185335,
     "end_time": "2023-05-27T22:34:18.477651",
     "exception": false,
     "start_time": "2023-05-27T22:34:07.292316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"lighteternal/wav2vec2-large-xlsr-53-greek\",\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    finetuning_task=\"wav2vec2_emotion_ru\",\n",
    "    pooling_mode=\"mean\"\n",
    ")\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", config=config)\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    max_steps=12000,\n",
    "    warmup_steps=500,\n",
    "    save_steps=4000,\n",
    "    eval_steps=4000,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936eca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:34:18.496279Z",
     "iopub.status.busy": "2023-05-27T22:34:18.494742Z",
     "iopub.status.idle": "2023-05-27T22:34:18.511271Z",
     "shell.execute_reply": "2023-05-27T22:34:18.510440Z"
    },
    "papermill": {
     "duration": 0.027992,
     "end_time": "2023-05-27T22:34:18.513441",
     "exception": false,
     "start_time": "2023-05-27T22:34:18.485449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        if info.free >= min_memory_available:\n",
    "            break\n",
    "        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490e512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:34:18.529944Z",
     "iopub.status.busy": "2023-05-27T22:34:18.529294Z",
     "iopub.status.idle": "2023-05-27T22:34:18.536879Z",
     "shell.execute_reply": "2023-05-27T22:34:18.536053Z"
    },
    "papermill": {
     "duration": 0.018114,
     "end_time": "2023-05-27T22:34:18.538737",
     "exception": false,
     "start_time": "2023-05-27T22:34:18.520623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "\n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        min_memory_available = 2 * 1024 * 1024 * 1024\n",
    "        clear_gpu_memory()\n",
    "        wait_until_enough_gpu_memory(min_memory_available)\n",
    "        \n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  # set_epoch() is handled by the Trainer\n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a6f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:34:18.554942Z",
     "iopub.status.busy": "2023-05-27T22:34:18.554233Z",
     "iopub.status.idle": "2023-05-27T22:34:28.853227Z",
     "shell.execute_reply": "2023-05-27T22:34:28.852037Z"
    },
    "papermill": {
     "duration": 10.309754,
     "end_time": "2023-05-27T22:34:28.855821",
     "exception": false,
     "start_time": "2023-05-27T22:34:18.546067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dusha[\"train\"],\n",
    "    eval_dataset=dusha[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[ShuffleCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593aee99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T22:34:28.872446Z",
     "iopub.status.busy": "2023-05-27T22:34:28.872109Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-27T22:34:28.863814",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321896f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"KELONMYOSA/dusha_emotion_audio\",\n",
    "    \"dataset\": \"Dusha\",\n",
    "    \"language\": \"ru\",\n",
    "    \"model_name\": \"Speech emotion recognition\",\n",
    "    \"finetuned_from\": \"facebook/wav2vec2-xls-r-300m\",\n",
    "    \"tasks\": \"emotion-speech-recognition\",\n",
    "    \"tags\": \"emotion\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-27T22:33:26.002034",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
